package com.akolar.eventproc.consumer

import java.util.Properties

import com.akolar.eventproc.{Config, Country}
import com.akolar.eventproc.consumer.MeetupEvent.{Event, Location}
import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.node.ObjectNode
import org.apache.flink.shaded.jackson2.com.fasterxml.jackson.dataformat.csv.CsvSchema
import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.functions.timestamps.BoundedOutOfOrdernessTimestampExtractor
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.assigners.{TumblingEventTimeWindows, TumblingProcessingTimeWindows}
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.Semantic
import org.apache.flink.streaming.connectors.kafka.{FlinkKafkaConsumer, FlinkKafkaProducer, KafkaSerializationSchema}
import org.apache.flink.streaming.util.serialization.JSONKeyValueDeserializationSchema

object MeetupJob {

  @throws[Exception]
  def main(args: Array[String]): Unit = {
    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    env.enableCheckpointing(10000L)
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)

    val stream = env.addSource(getConsumer())

    val input = stream
      .map {v => v.get("value")}
      .filter {_.has("venue")}
      .filter {v => {
        val venue = v.get("venue")
        v.has("time") &&
          venue.has("country") &&
          venue.has("city") &&
          venue.has("lat") &&
          venue.has("lon")
      }}
      .map {v => {
          val venue = v.get("venue")
          val country = venue.get("country").asText.toUpperCase
          val city = venue.get("city").asText.toLowerCase
          val lat = venue.get("lat").asDouble
          val lon = venue.get("lon").asDouble
          val evTime = v.get("time").asLong

          new Event(country, city, new Location(lat, lon), evTime)
        }}
        .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor[Event](Time.hours(1)){
          override def extractTimestamp(t: Event): Long = { t.EventTime }
        })

    input
      .filter {_.country == "DE"}
      .filter {v => {v.city == "munich" || v.city == "mÃ¼nchen" || v.city == "muenchen"}}
      .map {v => {
            val latBucket = (v.loc.lat / 0.009).toInt
            val lonBucket = (v.loc.lon / 0.0134).toInt
            (f"$latBucket,$lonBucket", 1)
        }}
      .keyBy(0)
      .sum(1)
      .addSink(getProducer(Config.KafkaMunichHotspotsTopic))

    input.filter { v => Country.inEurope(v.country) }
        .map {v => (v.city, 1)}
        .keyBy(0)
        .window(TumblingEventTimeWindows.of(Time.days(30)))
        .sum(1)
        .addSink(getProducer(Config.KafkaTopCitiesTopic))

    env.execute("meetup pipeline")
  }

  def getConsumer(): FlinkKafkaConsumer[ObjectNode] = {
    val properties = new Properties
    properties.put("bootstrap.servers", Config.KafkaURI)
    properties.setProperty("group.id", Config.KafkaConsumerGroup)

    val consumer = new FlinkKafkaConsumer(
      Config.KafkaMeetupEventsTopic,
      new JSONKeyValueDeserializationSchema(false),
      properties)
    consumer.setStartFromEarliest()

    consumer
  }

  def getProducer(topic: String): FlinkKafkaProducer[(String, Int)] = {
    val properties = new Properties
    properties.put("bootstrap.servers", Config.KafkaURI)
    properties.put("transaction.max.timeout.ms", "900000")
    properties.put("transaction.timeout.ms", "600000")
    new FlinkKafkaProducer[(String, Int)](topic, new AggregateSerializer(topic), properties, Semantic.AT_LEAST_ONCE)
  }
}
